{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intro\n",
    "\n",
    "machine learning = subdomain of CS \n",
    "- focuses on algorithms which help a computer learn from data without explicit programming\n",
    "- subset of AI, tries solve specific problems and make predictions using data\n",
    "\n",
    "artifical inteligence = area of CS\n",
    "- goal is to enable computers and machines perform human like tasks and simulate human behaviour\n",
    "\n",
    "data science = find patterns and draw insights from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### types of ML:\n",
    "- supervised learning:\n",
    "    - uses labeled inputs (input has corresponding output label) to train models and learn outputs\n",
    "- unsupervised learning:\n",
    "    - uses unlabeled data to learn patterns in data\n",
    "- reinforcement learning:\n",
    "    - agent learning in interactive environment based on rewards and penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml model:\n",
    "feature vector &emsp;&emsp;&ensp;&emsp; &emsp;  types of predictions\\\n",
    "input 1 &emsp;&emsp;   -> \\\n",
    ". . . &ensp; &emsp; &emsp;&emsp;  -> `model` -> output (prediction) \\\n",
    "input n &emsp;&emsp;  ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features:\n",
    "- qualitative:\n",
    "    - categorical data (finite number of categories or groups)\n",
    "    - nominal data (no inherent order built into the categorical data) i.e. countries do not have an order\n",
    "    - feed nominal data into computer using one-hot encoding\n",
    "        - tells the computer if a value matches some category, return '1', else return '0' \\\n",
    "    - different age groups (baby, toddler, child, young adult, adult, senior, etc. ) = ordinal data\n",
    "    - ordinal data = inherent order built into this category, i.e. toddler is younger than child.\n",
    "    - can mark from 1-5, whereas with countries, it cant be ranked/rated\n",
    "- quantitative: \n",
    "    - numerical valued data (discrete = integers or continuous = any real number)\n",
    "    -e.g. length of something, temperature = continuous\n",
    "    -e.g. number of things = discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supervised learning tasks:\n",
    "- classification : predict discrete classes\n",
    "    - multiclass classification: discrete classes the model tries putting the data into, i.e. plant species\n",
    "    - binary classification : two categories, i.e.  positive/negative, yes/no\n",
    "- regression : predict continuous values\n",
    "    - i.e predict the price of ETH tomorrow or temperature tomorrow\n",
    "    - predict a number as close to true value as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supervised learning dataset:\n",
    "- have to asess how well the model can generalize\n",
    "- split data set -> training data set, validation dataset, testing dataset\n",
    "- training dataset: tinker with a model till the output is as true to the true value with every iteration\n",
    "    - loss/losses = difference between prediction and true values (numerical quantity)\n",
    "    - make adjustments (training)\n",
    "- validation set: used as a reality check during or after training to ensure model can handle unseen data\n",
    "    - loss never gets fed back into model, feedback loop is not closed\n",
    "- losses:\\\n",
    "input 1 -> `MODEL A` -> Output + Loss = 1.3\\\n",
    "input 1 -> `MODEL B` -> Output + Loss = 1.5\\\n",
    "input 1 -> `MODEL C` -> Output + Loss = 0.5\\\n",
    "input 1 -> `MODEL D` -> Output + Loss = 0.9\n",
    "    - `MODEL C` has least loss, so will be used\n",
    "    - test set used as to check how generaliizable the final chosen model is \n",
    "    - test set -> `MODEL C` -> output [loss = `final reported performance` of model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics of performance:\n",
    "- loss functions\n",
    "    - L1 loss: \\\n",
    "    $loss=sum(|y_{real} - y_{predicted}|)$ \\\n",
    "        <img src=\"https://tse4.mm.bing.net/th/id/OIP.EoiU6FhK_9adaX4BsSpnkwHaDU?pid=ImgDet&rs=1\" width=\"400\" height=\"200\" />\n",
    "    - L2 loss: \\\n",
    "    $loss=sum((y_{real} - y_{predicted})^2)$ \\\n",
    "        <img src=\"https://tse4.mm.bing.net/th/id/OIP.mjQoX8ChUoeravI45aEw-QHaDX?pid=ImgDet&rs=1\" width=\"400\" height=\"200\" />\n",
    "    - binary cross-entropy loss: loss decrease as performance increase \\\n",
    "    $loss= -\\frac{1}{N}*sum(y_{real}*log(y_{predicted})+(1-y_{real})*log((1-y_{predicted})))$\n",
    "- accuracy\n",
    "    - correct predictions/ actual *100 (as a percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbours (KNN):\n",
    "-  uses proximity to make classifications or predictions about the grouping of an individual data point\n",
    "- define distance and classify some points\n",
    "- Euclidian distance, $d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 } $\\\n",
    "\n",
    "<img src=\"https://matlab1.com/wp-content/uploads/2017/11/knn-concept.jpg\" width=\"300\" height=\"200\" /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision and recall\n",
    "\n",
    "- true positive = identified correctly by model and actually true\n",
    "- false positive = incorrectly identified by model as true but actually false\n",
    "- false negative = incorrectly identified by model as false but actually true (mjissed values)\n",
    "- true negative = identified correctly by model as false and correctly excluded\n",
    "\n",
    "- precision = true positive / (true positive + false positive)\n",
    "- recall =  true positive / (true positive + false negative)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" width=\"300\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes\n",
    "\n",
    "bayes rule: \n",
    "- describes probability of an event based on prior knowledge of conditions\n",
    "- $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "$posterior = \\frac{likelihood \\times prior}{evidence}$\n",
    "\n",
    "naive bayes: \n",
    "- conditional probability model\n",
    "- assigns probabilities $p(C_k \\mid x_1, \\ldots, x_n)$ \n",
    "- for each possible $K$ outcome \n",
    "- or for classes $C_k$ \n",
    "- given a problem represented by feature vector $\\mathbf{x}=(x_1, ..., x_2)$\n",
    "- encoding some $n$ features\n",
    "\n",
    "$p(C_k | \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid C_k) \\cdot p(C_k)}{p(\\mathbf{x})} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in practice, only interested in numerator as denomator does not depend on $C$ and features $x_i$ \n",
    "- therefore, denominator = constant\n",
    "\n",
    "- numerator = joint probability model = $p(C_k, x_1, ..., x_n)$\n",
    "- rewritten using chain rule:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "p(C_k, x_1, \\ldots, x_n) & = p(x_1, \\ldots, x_n, C_k) \\\\\n",
    "                        & = p(x_1 \\mid x_2, \\ldots, x_n, C_k) \\ p(x_2 \\mid x_3, \\ldots, x_n, C_k) \\cdots   p(x_{n-1} \\mid x_n, C_k) \\ p(x_n \\mid C_k) \\ p(C_k) \\\\\n",
    "\\end{align}\\\n",
    "$\n",
    "\n",
    "- 'naive' conditional independence assumptions:\n",
    "    - assume that all features in $\\mathbf{x}$ are mutually independent, conditional on category $C_k$, under assumption:\\\n",
    "        $p(x_i \\mid x_{i+1}, \\ldots ,x_{n}, C_k ) = p(x_i \\mid C_k)$\n",
    "\n",
    "- joint model can be expressed as\n",
    "$\n",
    "\\begin{align}\n",
    "p(C_k \\mid x_1, \\ldots, x_n) \\varpropto\\ & p(C_k, x_1, \\ldots, x_n) \\\\\n",
    "                            & = p(C_k) \\ p(x_1 \\mid C_k) \\ p(x_2\\mid C_k) \\ p(x_3\\mid C_k) \\ \\cdots \\\\\n",
    "                            & = p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k)\\,,\n",
    "\\end{align}\\\n",
    "$\n",
    "\n",
    "- where $\\varpropto$ denotes proportionality bc omitted denominator $p(\\mathbf{x})$\n",
    "\n",
    "- constructing a classifier from probability model:\n",
    "    - classifier combined naive bayes model with decision rule\n",
    "    - bayes classifier is function that assigns a class label $\\hat{y} =C_k $\n",
    "    - MAP decision rule rule = pick hypothesis most probable, minimize missclassification\n",
    "\n",
    "    $\\hat{y} = \\underset{k \\in \\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\ p(C_k) \\displaystyle\\prod_{i=1}^n p(x_i \\mid C_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression\n",
    "\n",
    "Logistic function of the form:\n",
    "- $p(x) = \\frac{1}{1+e^{-(x-\\mu) /s}}$\n",
    "- $p(x) = \\frac{1}{1+e^{-(mx+b)}} = S(mx+b)$\n",
    "\n",
    "<img src=\"https://www.machinelearningplus.com/wp-content/uploads/2017/09/linear_vs_logistic_regression.jpg\" width=\"500\" height=\"250\">\n",
    "\n",
    "Sigmoid Function:\n",
    "- $S(y) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "<img src=\"https://tse3.mm.bing.net/th/id/OIP.BwZmaMBaVW8f8lBAQUoytwHaE7?pid=ImgDet&rs=1\" width=\"300\" height=\"250\">\n",
    "\n",
    "- Simple logistic regression: $x_0$\n",
    "- Multiple logistic regression: $x_0, x_1, ... x_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### support vector machines (svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimal hyperplane in N-dimensional space that differentiates classes in feature space\n",
    "- hyperplane = decision boundary \n",
    "\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201211162942/Capture.JPG\" width=\"250\" height=\"250\" /> \n",
    "\n",
    "- multiple hyperplanes separating data\n",
    "\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201211181531/Capture.JPG\" width=\"250\" height=\"250\" /> \n",
    "- choose hyperplane whose distance from it to nearest data point on each side = maximised\n",
    "- support vector = closest data pts to hyperplane\n",
    "- margin = distance between support vector and hyperplane\n",
    "    - wider margin = better classification performance\n",
    "- hard margin = max margin hyperplane\n",
    "- soft margin = technique when data is not perfectly separable or countains outliers\n",
    "- regularisation parameter C: margin maixmisation and misclassification fines are balanced by C\n",
    "    - stricter penality -> greater value of C -> smaller margin\n",
    "- hinge loss = typical loss function in SVMs\n",
    "- dual problem of optimisation problem requires locating lagrange multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel trick : $x \\rightarrow (x, x^2)$\n",
    "    - non linearly separably 1d dataset \\\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201211183907/Capture.JPG\" width=\"300\" height=\"150\" />\n",
    "    - mapping 1d data to 2d to become able to separate between 2 classes \\\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201211185229/Capture.JPG\" width=\"300\" height=\"250\" />\n",
    "    - new variable y created as a function of distance from origin\n",
    "    - non linear function that creates a new variable = kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron = f(ax+b),\n",
    "- f = activation function\n",
    "- a = vector\n",
    "- b = bias\n",
    "- x = vector\n",
    "\n",
    "<img src=\"https://tse1.mm.bing.net/th/id/OIP.JmR8FXTrzwFMXTLP72k_owAAAA?pid=ImgDet&rs=1\" width=\"400\" height=\"200\" /> \n",
    "\n",
    "whole network of this = neural network\n",
    "\n",
    "\n",
    "<img src=\"https://assets.website-files.com/5d7b77b063a9066d83e1209c/60f040c91fd9cd1e45773b93_Neural-Network-Architectures-Cover%20(1).png\" width=\"300\" height=\"300\" /> \n",
    "\n",
    "activation function: without this the neural net would be a linear combination of input layers\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"500\" height=\"250\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backpropagation\n",
    "\n",
    "Gradient descent : steps that gradually get smaller as approaches min. point\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/vincent1bt/image/upload/c_scale,w_523/v1549328042/gradient_descent_snoiq9.jpg\" width=\"250\" height=\"250\" /> \n",
    "\n",
    "$w_{0, new} = w_{0, old} + \\alpha^*$ \n",
    "\n",
    "$ \\alpha^*$  = learning rate, controlling how long it takes for neural net to converge / diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML libraries\n",
    "\n",
    "- tensorflow: makes it easy to define model, open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple linear regression\n",
    "= fitting linear model to data \n",
    "assumptions:\n",
    "- linearity : does data follow linear pattern \n",
    "- independence : all data points do not effect each other \n",
    "- normality : residuals should be normally distributed around lobf\n",
    "- homoscedasticity : variants of points should remain constant\n",
    "\n",
    "<img src=\"https://www.skysilk.com/blog/wp-content/uploads/2018/09/SimpleLinearRegressionEquation.jpg\" width=\"500\" height=\"400\" /> \n",
    "\n",
    "- residual / error = $|y_i - \\hat{y}_i|$ = |point - prediction|\n",
    "- line of best fit / predictor = $y=b_0 + b_1 x$\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/691/1*bEJ9goEgOKQ8VPv72eb-8Q.png\" width = \"300\" height = \"250\" >\n",
    "\n",
    "- aim is to reduce $\\sum_{i} |y_i - \\hat{y}_i| $\n",
    "- try find $b_0$ and $b_1$ that achieves this ^ or $\\sum_{i} |y_i - \\hat{y}_i| ^2 $\n",
    "\n",
    "- multiple linear regression\n",
    "    - line of best fit / predictor = $y=b_0 + b_1 x_1 + b_2 x_2 + ... + ... b_n x_n $\n",
    "\n",
    "- linear regression model evaluation techniques\n",
    "    - mean absolute error (MAE)\n",
    "\n",
    "         <img src = \"https://jmlb.github.io/images/20180701/img_01.png\" width = \"250\" height = \"200\">\n",
    "         \n",
    "        - $MAE = \\frac{\\sum^{n}_{i} |y_i - \\hat{y}_i|}{n} $\n",
    "\n",
    "    - mean square error (MSE)\n",
    "        - helps punish large errors\n",
    "        - functions that are differentiable, whereas abs value isnt differentiable everywhere\n",
    "        - $MSE = \\frac{\\sum^{n}_{i} |y_i - \\hat{y}_i|^2}{n} $\n",
    "        - gets difficult to compare values, as MSE is in terms of $y^2$, so observe the squared of how off it is\n",
    "\n",
    "    - root mean square error (MRSE)\n",
    "        - helps compare values, same power/ factor\n",
    "        - $RMSE = \\sqrt{\\frac{\\sum^{n}_{i} |y_i - \\hat{y}_i|^2}{n}} $\n",
    "        - gets difficult to compare values, as MSE is in terms of $y^2$, so observe the squared of how off it is\n",
    "\n",
    "    - $R^2$ coefficient of determination\n",
    "        - $R^2 = 1- \\frac{RSS}{TSS}$ $\\rightarrow 1 $ = good indicator of a good predictor \n",
    "        - $RSS$ = sum of squared residiuals = ${\\sum^{n}_{i} (y_i - \\hat{y}_i)^2}$\n",
    "        - $TSS$ = total sum of squares = ${\\sum^{n}_{i} (y_i - \\bar{y}_i)^2}$\n",
    "\n",
    "        <img src = \"https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2016/12/anat.png\" width = '500' height = '350'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "- compute k clusters from data\n",
    "- k = predefined by user\n",
    "\n",
    "- <img src= \"https://serokell.io/files/q4/q49pm3tx.K-Means_Clustering_Algorithm_pic1_(1).png\" width= \"500\" height = \"300\">\n",
    "\n",
    "- for k = n:\n",
    "    - choose n random points to be centroids\n",
    "    - calculate distance between all points and centroids\n",
    "    - assigning points to closest centoid -> expectation step\n",
    "    - compute new centroids -> maximisation\n",
    "    - redo previous step, aka calc distance between points and centroids\n",
    "    - converge to stable solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### principal component analysis\n",
    "- dimensionality reduction \n",
    "- component (direction in space) with largets variance\n",
    "- minimise projection residuals\n",
    "- or maximising variance\n",
    "\n",
    "<img src = \"https://media.licdn.com/dms/image/D5622AQGQ0ePUkr69Dw/feedshare-shrink_800/0/1696532305367?e=1699488000&v=beta&t=UDwmkwK-dDGaxJD9eQ7cCEYoMf0XzkVOVHHeHZHD5gE\" width = \"250\" height = \"200\">\n",
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20230420165431/Principal-Componenent-Analysisi.webp\" height = \"200\" width = \"350\">\n",
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20230420165637/Finding-Projection-in-PCA.webp\" height = \"200\" width = \"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
